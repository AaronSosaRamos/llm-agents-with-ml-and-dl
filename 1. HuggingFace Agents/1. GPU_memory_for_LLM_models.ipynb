{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GPU memory for LLM models\n",
        "Made by: Wilfredo Aaron Sosa Ramos"
      ],
      "metadata": {
        "id": "-Hl0RUNyOqI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhoMUGsIOaBy",
        "outputId": "011bc878-6130-4ec2-f3ca-26a0c591a871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting get_gpu_memory.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile get_gpu_memory.py\n",
        "from typing import Dict, Union\n",
        "from huggingface_hub import get_safetensors_metadata\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "# Example:\n",
        "# python get_gpu_memory.py Owen/Qwen2.5-7B-Instruct\n",
        "\n",
        "# Dictionary mapping dtype strings to their byte sizes\n",
        "bytes_per_dtype = {\"int4\": 0.5, \"int8\": 1, \"float8\": 1, \"float16\": 2, \"float32\": 4}\n",
        "\n",
        "def calculate_gpu_memory(parameters: float, bytes: float) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the GPU memory required for serving a Large Language Model (LLM).\n",
        "    This function estimates the GPU memory needed using the formula:\n",
        "    M = (P * 4B) / (32 / Q) * 1.18\n",
        "    where:\n",
        "    - M is the GPU memory in Gigabytes\n",
        "    - P is the number of parameters in billions (e.g., 7 for a 7B model)\n",
        "    - 4B represents 4 bytes per parameter\n",
        "    - 32 represents bits in 4 bytes\n",
        "    - Q is the quantization bits (e.g., 16, 8, or 4 bits)\n",
        "    - 1.18 represents ~18% overhead for additional GPU memory requirements\n",
        "\n",
        "    Args:\n",
        "        parameters: Number of model parameters in billions\n",
        "        bytes: Number of bytes per parameter based on dtype\n",
        "\n",
        "    Returns:\n",
        "        Estimated GPU memory required in Gigabytes\n",
        "    \"\"\"\n",
        "    memory = round((parameters * 4) / (32 / (bytes * 8)) * 1.18, 2)\n",
        "    return memory\n",
        "\n",
        "def get_model_size(model_id: str, dtype: str = \"float16\") -> Union[float, None]:\n",
        "    \"\"\"\n",
        "    Get the estimated GPU memory requirement for a Hugging Face model.\n",
        "\n",
        "    Args:\n",
        "        model_id: Hugging Face model ID (e.g., \"facebook/opt-350m\")\n",
        "        dtype: Data type for model loading (\"float16\", \"int8\", etc.)\n",
        "\n",
        "    Returns:\n",
        "        Estimated GPU memory in GB, or None if estimation fails\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if dtype not in bytes_per_dtype:\n",
        "            raise ValueError(\n",
        "                f\"Unsupported dtype: {dtype}. Supported types: {list(bytes_per_dtype.keys())}\"\n",
        "            )\n",
        "\n",
        "        metadata = get_safetensors_metadata(model_id)\n",
        "        if not metadata or not metadata.parameter_count:\n",
        "            raise ValueError(f\"Could not fetch metadata for model: {model_id}\")\n",
        "\n",
        "        model_parameters = list(metadata.parameter_count.values())[0]\n",
        "        model_parameters = int(model_parameters) / 1_000_000_000  # Convert to billions\n",
        "        return calculate_gpu_memory(model_parameters, bytes_per_dtype[dtype])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error estimating model size: {str(e)}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"model_id\", help=\"Hugging Face model ID (e.g., Qwen/Qwen2.5-7B-Instruct)\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dtype\",\n",
        "        default=\"float16\",\n",
        "        choices=bytes_per_dtype.keys(),\n",
        "        help=\"Data type for model loading\",\n",
        "    )\n",
        "    args = parser.parse_args()\n",
        "    size = get_model_size(args.model_id, args.dtype)\n",
        "    print(\n",
        "        f\"Estimated GPU memory requirement for {args.model_id}: {size:.2f} GB ({args.dtype})\"\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_gpu_memory.py microsoft/phi-4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZCMnn_aPXin",
        "outputId": "e950798a-fd45-40c9-bc3d-11f85a08beef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parse safetensors files: 100% 6/6 [00:00<00:00, 29.70it/s]\n",
            "Estimated GPU memory requirement for microsoft/phi-4: 34.60 GB (float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_gpu_memory.py deepseek-ai/DeepSeek-V3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXP3LSXjPrTi",
        "outputId": "aa575eaa-ae64-462c-a1e1-9fda83d41955"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model.safetensors.index.json: 100% 8.90M/8.90M [00:00<00:00, 23.5MB/s]\n",
            "Parse safetensors files: 100% 163/163 [00:03<00:00, 53.33it/s]\n",
            "Estimated GPU memory requirement for deepseek-ai/DeepSeek-V3: 0.10 GB (float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python get_gpu_memory.py jinaai/ReaderLM-v2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAIDhxN5RHOh",
        "outputId": "081715b1-203d-48eb-dfea-bcdbf729accf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated GPU memory requirement for jinaai/ReaderLM-v2: 4.19 GB (float16)\n"
          ]
        }
      ]
    }
  ]
}